name: "text_recognition"
backend: "onnxruntime"
max_batch_size : 0
input [
  {
    name: "x"
    data_type: TYPE_FP32
    dims: [ -1,3,48,-1]
  }
]
output [
  {
    name: "softmax_2.tmp_0"
    data_type: TYPE_FP32
    dims: [ -1, -1, 97 ]
  }
]
#dynamic_batching { }

instance_group [
    {
      count: 10
      kind: KIND_GPU
    }
]

optimization {
  execution_accelerators {
    gpu_execution_accelerator: [{
      name: "tensorrt"
      parameters { key: "precision_mode" value: "FP16" }
      parameters { key: "trt_engine_cache_enable" value: "1" }
      parameters { key: "trt_engine_cache_path"   value: "/workspace/models/_trt_cache/text_recognition" }
      parameters { key: "trt_cuda_graph_enable"   value: "0"}
      parameters { key: "max_workspace_size_bytes" value: "4294967296"}
    }]
  }
}

parameters { key: "execution_mode"        value: { string_value: "1" } }
parameters { key: "intra_op_thread_count" value: { string_value: "0" } }
parameters { key: "inter_op_thread_count" value: { string_value: "0" } }

# (Optional) Warmup â€“ pick a representative width your pipeline produces
# model_warmup: [
#   {
#     name: "warm1"
#     count: 1
#     inputs: [
#       { key: "x" data_type: TYPE_FP32 dims: [ 1, 32, 256 ] zero_data: true }
#     ]
#   }
# ]
